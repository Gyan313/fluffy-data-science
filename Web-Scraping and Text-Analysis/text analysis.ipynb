{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9803d4a9",
   "metadata": {},
   "source": [
    "## Going to find the variable values give in text analysis pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive and negative score....\n",
    "\n",
    "# getting the positive provided in assaignment..\n",
    "with open(\"./MasterDictionary-20231016T012001Z-001/MasterDictionary/positive-words.txt\",\"r\") as file:\n",
    "    positive_words = file.readlines()\n",
    "\n",
    "# getting the negative provided in assaignment...\n",
    "with open(\"./MasterDictionary-20231016T012001Z-001/MasterDictionary/negative-words.txt\",\"r\") as file:\n",
    "    negative_words = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = [word.strip() for word in positive_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f76d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = [word.strip() for word in negative_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the tokenized_clean_article\n",
    "with open(\"./Cleaned-Articles/10282.6.txt\",\"r\",encoding='utf-8') as file:\n",
    "    article_tokens = file.readlines()\n",
    "\n",
    "article_tokens = [word.strip() for word in article_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tokens_article = [(word,-1) for word in article_tokens if word in negative_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea214a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tokens_article[1][1]\n",
    "# here numpy changes all the -1 to str(-1). That's why we used astype(int) to change str(-1) to int(-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfe25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(negative_tokens_article[:,1]).astype(int).sum()*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(negative_tokens_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5925e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2,'POSITIVE SCORE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e0082",
   "metadata": {},
   "source": [
    "## Merged Below \"Sentimental_Analysis\" function code and cell code to RealCode cell...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing function to fill number of positive and negative words in df ====> dataframe.\n",
    "# lets calculate the 'Polarity Score' and 'Subjectivity Score' alongside +ve and -ve score.....\n",
    "# Polarity Score = (Positive Score ‚Äì Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "# Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "\n",
    "# modules to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")\n",
    "\n",
    "# changing the index of df to 'URL_ID' because there are 2 rows in df which will remain nan in every column....\n",
    "df.set_index(\"URL_ID\",inplace=True)\n",
    "\n",
    "def Sentimental_Analysis(positive_words_path:str,negative_words_path:str):\n",
    "    \n",
    "    positive_words = get_positive_words(positive_words_path)\n",
    "    negative_words = get_negative_words(negative_words_path)\n",
    "    \n",
    "    clean_article = os.listdir(\"Cleaned-Articles/\")\n",
    "    \n",
    "    for article in clean_article:\n",
    "        print(f\"{article} ---> the article we are onüôÉ‚ù§Ô∏è\")\n",
    "        # opening the article file to get positive_score and negative_score\n",
    "        with open(f\"./Cleaned-Articles/{article}\",\"r\",encoding='utf-8') as file:\n",
    "            article_tokens = file.readlines()\n",
    "            article_tokens = [word.strip() for word in article_tokens]\n",
    "        \n",
    "        # calculate positive_score and negative_score\n",
    "        # positive_score...\n",
    "        positive_tokens = np.array([[word,1] for word in article_tokens if word in positive_words])\n",
    "        positive_score = 0\n",
    "        if positive_tokens.size != 0:\n",
    "            positive_score = (positive_tokens[:,1]).astype(int).sum()\n",
    "        \n",
    "        # negative_score.....\n",
    "        negative_tokens = np.array([[word,-1] for word in article_tokens if word in negative_words])\n",
    "        negative_score = 0\n",
    "        if negative_tokens.size != 0:\n",
    "            negative_score = (negative_tokens[:,1]).astype(int).sum()*-1\n",
    "        \n",
    "        # putting the negative_score and positive_score in the df....\n",
    "        row = float(article.replace(\".txt\",\"\"))\n",
    "        polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score)/ ((len(article_tokens)) + 0.000001)\n",
    "        df.loc[row,'POSITIVE SCORE'] = positive_score\n",
    "        df.loc[row,'NEGATIVE SCORE'] = negative_score\n",
    "        df.loc[row,'POLARITY SCORE'] = np.round(polarity_score,2)\n",
    "        df.loc[row,'SUBJECTIVITY SCORE'] = np.round(subjectivity_score,2)\n",
    "        print(f\"{row}th row's +ve and -ve score is filled successfullyüòäüòé\")\n",
    "    \n",
    "def get_positive_words(path:str)->list:\n",
    "    # getting the positive provided in assaignment..\n",
    "    with open(path,\"r\") as file:\n",
    "        positive_words = file.readlines()\n",
    "    positive_words = [word.strip() for word in positive_words ]\n",
    "    return positive_words\n",
    "\n",
    "def get_negative_words(path:str)->list:\n",
    "    # getting the negative provided in assaignment...\n",
    "    with open(path,\"r\") as file:\n",
    "        negative_words = file.readlines()\n",
    "    negative_words = [word.strip() for word in negative_words ]\n",
    "    return negative_words\n",
    "\n",
    "def get_tokenized_article(path:str)->list:\n",
    "    with open(path,\"r\",encoding='utf-8') as file:\n",
    "        article_tokens = file.readlines()\n",
    "    article_tokens = [word.strip() for word in article_tokens]\n",
    "    return article_tokens\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        Sentimental_Analysis(positive_words_path=\"MasterDictionary-20231016T012001Z-001/MasterDictionary/positive-words.txt\",\n",
    "                          negative_words_path=\"MasterDictionary-20231016T012001Z-001/MasterDictionary/negative-words.txt\")\n",
    "        print()\n",
    "        print(\"############################################\")\n",
    "        \n",
    "        # updating the 'Output Data Structure.xlsx' \n",
    "        df.to_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")\n",
    "        \n",
    "        print(\"Output Data Structure.xlsx ----> file is updated succeessfully......\")\n",
    "        print(\"Everything happened seemlessly so move to next task of assaignment and remember..üò§ be ruthless...ü•∑\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d166ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(os.listdir(\"Cleaned-Articles/\")[0].replace(\".txt\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['POSITIVE SCORE'].isna()]\n",
    "\n",
    "# 11668.0\n",
    "# 11668.0\n",
    "# both 'URL_ID' contain no article...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d28d99",
   "metadata": {},
   "source": [
    "## This is the REALCODE cell..... Mazedar Comdingüòä‚ù§Ô∏è‚ù§Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b856dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Text Analysis ----> calculated all the variables or column values mentioned in the Text Analysis.docx\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import syllables\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")\n",
    "df.set_index(\"URL_ID\",inplace=True)\n",
    "# created a generic function which can fill the col of the dataframe with the values provided.\n",
    "# other than \"sentimental Analysis\" i donot need the \"cleaned_articles\" ---> (articles free from stopWords)....\n",
    "\n",
    "def add_df_col_values(column:str,function)->None:\n",
    "    print(f\"############## {function}() ----> is started executing ###############\")\n",
    "    \n",
    "    if function == positive_score or function == negative_score or function == subjectivity_score:\n",
    "        articles_list = os.listdir(f\"Cleaned-Articles/\")[1:]\n",
    "    else:\n",
    "        articles_list = os.listdir(f\"articles_text/\")[1:]\n",
    "    \n",
    "    for article in articles_list:\n",
    "        row = float(article.replace(\".txt\",\"\"))\n",
    "        \n",
    "        # opening the article files......\n",
    "        if function == positive_score or function == negative_score or function == subjectivity_score:\n",
    "            with open(f\"./Cleaned-Articles/{article}\",\"r\",encoding='utf-8') as file:\n",
    "                article_tokens = file.readlines()\n",
    "                article_tokens = [word.strip() for word in article_tokens]\n",
    "            df.loc[row,column] = function(article_tokens)\n",
    "        else:\n",
    "            with open(f\"./articles_text/{article}\",\"r\",encoding='utf-8') as file:\n",
    "                raw_article_text = file.read()\n",
    "            df.loc[row,column] = function(raw_article_text)\n",
    "    print(f\"############## {function}() ----> is executed successfully, moving on to next function ###############\")\n",
    "        \n",
    "# 1) POSITIVE SCORE\n",
    "def positive_score(article_tokens:list)->int:\n",
    "    # getting the positive provided in assaignment..\n",
    "    with open(\"MasterDictionary-20231016T012001Z-001/MasterDictionary/positive-words.txt\",\"r\") as file:\n",
    "        positive_words = file.readlines()\n",
    "        positive_words = [word.strip() for word in positive_words ]\n",
    "    \n",
    "    # calculating the positive score....\n",
    "    # positive_score...\n",
    "    positive_tokens = np.array([[word,1] for word in article_tokens if word in positive_words])\n",
    "    positive_score = 0\n",
    "    if positive_tokens.size != 0:\n",
    "        positive_score = (positive_tokens[:,1]).astype(int).sum()\n",
    "    \n",
    "    return positive_score\n",
    "\n",
    "# 2) Negative Score\n",
    "def negative_score(article_tokens:list)->int:\n",
    "    # getting the negative provided in assaignment...\n",
    "    with open(\"MasterDictionary-20231016T012001Z-001/MasterDictionary/negative-words.txt\",\"r\") as file:\n",
    "        negative_words = file.readlines()\n",
    "        negative_words = [word.strip() for word in negative_words ]\n",
    "    \n",
    "    # calculating the negative score.....\n",
    "    # negative_score.....\n",
    "    negative_tokens = np.array([[word,-1] for word in article_tokens if word in negative_words])\n",
    "    negative_score = 0\n",
    "    if negative_tokens.size != 0:\n",
    "        negative_score = (negative_tokens[:,1]).astype(int).sum()*-1\n",
    "    \n",
    "    return negative_score\n",
    "        \n",
    "    \n",
    "# 3) Polarity Score:\n",
    "# Polarity Score = (Positive Score ‚Äì Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "# \"Polarity Score\" can be found easily using pandas dataframe properties....\n",
    "\n",
    "# 4) Subjectivity Score: \n",
    "# Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "def subjectivity_score(article_tokens:list)->float:\n",
    "    pos_score = positive_score(article_tokens)\n",
    "    neg_score = negative_score(article_tokens)\n",
    "    score = np.round((pos_score + neg_score)/ ((len(article_tokens)) + 0.000001),2)\n",
    "    return score\n",
    "\n",
    "\n",
    "# 5) calcualte the 'Average Sentence Length'\n",
    "def average_Sentence_Length(article_text):\n",
    "    sent_tokens = sent_tokenize(article_text)\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    avg_sent_length = len(word_tokens) / len(sent_tokens)\n",
    "    return int(np.round(avg_sent_length))\n",
    "\n",
    "# 6) Percentage of Complex words = the number of complex words / the number of words \n",
    "# all the edge cases for finding the number of syllables in words are being handled in the 'python library syllables'.\n",
    "def percentage_of_complex_words(article_text):\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    complex_words = [word for word in word_tokens if syllables.estimate(word)>2]\n",
    "    percentage_complex_words = (len(complex_words) / len(word_tokens))*100\n",
    "    return np.round(percentage_complex_words,2)\n",
    "    \n",
    "\n",
    "# 7) To calculate ----> Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "# but we donot need to create the fog_index function to calculate it because pandas can take care of it much easily....\n",
    "\n",
    "# 8) Average Number of Words Per Sentence\n",
    "# when calculated for the same text, the values for average sentence length and average number of words per sentence will be \n",
    "# the same. These two metrics provide different insights into the text, but because they are derived from the same data \n",
    "# (the text's words and sentences), they will yield identical numerical results.\n",
    "def average_num_words_per_sentence(article_text):\n",
    "    return average_Sentence_Length(article_text)\n",
    "\n",
    "# 9) Complex Word Count\n",
    "def complex_word_count(article_text):\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    complex_words = [word for word in word_tokens if syllables.estimate(word)>2]\n",
    "    return len(complex_words)\n",
    "\n",
    "# 10) Word Count\n",
    "# We count the total cleaned words present in the text by.....\n",
    "# i) removing the stop words (using stopwords class of nltk package).\n",
    "# ii) removing any punctuations like ? ! , . from the word before counting.\n",
    "def word_count(article_text):\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    # removing the stopwords...\n",
    "    word_tokens = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    # removing the punctuations.....\n",
    "    word_tokens = [word for word in word_tokens if word not in string.punctuation]\n",
    "    return len(word_tokens)\n",
    "\n",
    "\n",
    "# 11) Syllable Count Per Word\n",
    "def syllable_count_per_word(article_text):\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    syllables_per_word = np.array([syllables.estimate(word) for word in word_tokens])\n",
    "    return np.round(syllables_per_word.mean())\n",
    "\n",
    "# 12) Personal Pronouns\n",
    "#  ‚ÄúI,‚Äù ‚Äúwe,‚Äù ‚Äúmy,‚Äù ‚Äúours,‚Äù and ‚Äúus‚Äù and beware of US as it is not personal pronoun.\n",
    "# \"re.I\" means check string in a case insensitive manner\n",
    "# and (?-i:us) ----> inline modifier group where matching is CASE SENSITIVE, and this only matches 'us' (not 'US')\n",
    "def count_personal_pronouns(article_text):\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    regex = re.compile(r\"\\b(I|we|my|ours|(?-i:us))\\b\",re.I)\n",
    "    personal_pronouns = [word for word in word_tokens if regex.findall(word)]\n",
    "    return len(personal_pronouns)\n",
    "\n",
    "# 13) Average Word Length\n",
    "# Sum of the total number of characters in each word/Total number of words\n",
    "def average_word_length(article_text):\n",
    "    word_tokens = word_tokenize(article_text)\n",
    "    # removing the punctuations.....\n",
    "    word_tokens = [word for word in word_tokens if word not in string.punctuation]\n",
    "    avg_word_len = np.array([len(word) for word in word_tokens])\n",
    "    return np.round(avg_word_len.mean())\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    try:\n",
    "        add_df_col_values(\"POSITIVE SCORE\",positive_score)\n",
    "        add_df_col_values(\"NEGATIVE SCORE\",negative_score)\n",
    "        df['POLARITY SCORE'] = np.round((df['POSITIVE SCORE'] - df['NEGATIVE SCORE']) / ((df['POSITIVE SCORE'] + df['NEGATIVE SCORE']) + 0.000001),2) \n",
    "        add_df_col_values(\"SUBJECTIVITY SCORE\",subjectivity_score)\n",
    "        add_df_col_values(\"AVG SENTENCE LENGTH\",average_Sentence_Length)\n",
    "        add_df_col_values(\"PERCENTAGE OF COMPLEX WORDS\",percentage_of_complex_words)\n",
    "        df['FOG INDEX'] = np.round(0.4 * (df[\"AVG SENTENCE LENGTH\"] + df[\"PERCENTAGE OF COMPLEX WORDS\"]),2)\n",
    "        add_df_col_values(\"AVG NUMBER OF WORDS PER SENTENCE\",average_num_words_per_sentence)\n",
    "        add_df_col_values(\"COMPLEX WORD COUNT\",complex_word_count)\n",
    "        add_df_col_values(\"WORD COUNT\",word_count)\n",
    "        add_df_col_values(\"SYLLABLE PER WORD\",syllable_count_per_word)\n",
    "        add_df_col_values(\"PERSONAL PRONOUNS\",count_personal_pronouns)\n",
    "        add_df_col_values(\"AVG WORD LENGTH\",average_word_length)\n",
    "        \n",
    "        # reseting the integer range(0,113) as the index with following command....\n",
    "        df.reset_index(drop=False,inplace=True)\n",
    "        \n",
    "        # updating the 'Output Data Structure.xlsx' \n",
    "        df.to_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")\n",
    "        \n",
    "        print()\n",
    "        print(\"#####################################################\")\n",
    "        print(\"Everything happend seemlessly, be ruthless and you can do itü•∑‚ù§Ô∏è\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")\n",
    "df2.drop(['Unnamed: 0'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5493ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = os.listdir(\"articles_text\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a448bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.drop(['Unnamed: 0'],inplace=True,axis=1)\n",
    "(df == df2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf5e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_s_c = np.array([syllables.estimate(word) for word in word_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd989aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(mean_s_c.mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r\"\\b(I|we|my|ours|(?-i:us))\\b\",re.I)\n",
    "[word for word in word_tokens if regex.findall(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d66ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [word for word in word_tokens if word not in string.punctuation]\n",
    "avg_word_len = np.array([len(word) for word in word_tokens])\n",
    "np.round(avg_word_len.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles_text/123.0.txt\",\"r\",encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"SYLLABLE PER WORD\"] == np.nan).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a587e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FOG INDEX\"] = np.round(df[\"FOG INDEX\"],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81df7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b72af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72552bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4780783",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(percentage_of_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./articles_text/10282.6.txt\",\"r\",encoding='utf-8') as file:\n",
    "    article_tokens = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcd6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deec36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b21911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fdc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e57a846",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "\n",
    "> *Must visit `objective.docx` file to know the flow of the program from Data_Extraction to Data_Analysis and instructions to do every step of this Assaignment....\n",
    "In this file you will get know about all the files used in this Assaignment....*\n",
    "\n",
    "**Aim** - The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained thoroughly in `Text Analysis.docx` file. \n",
    "\n",
    "`Output Data Structure.xlsx` ------- contain the all the URL's and the variable we need to find in order to do Text Analysis.......\n",
    "And it is located inside the *Data directory*....\n",
    "\n",
    "-------- Lets go through each step of this assaignment one by one ----------\n",
    "\n",
    "1) **Data Extraction**\n",
    "\n",
    "    * Program for \"Data Extraction\" is in file **Data_Extraction.py** \n",
    "    * This program uses *BeautifulSoup* module to get text from the article.\n",
    "    * So ,I made two functions `get_article_text` and `store_article_text` and there purpose is pretty clear by there names.\n",
    "    * `get_article_text` --- this function uses the *BeautifulSoup* module to get the title and text of article.This function returns a list containing the *title* and *text of article*. \n",
    "    * `store_article_text` ---- this function stores the title and text we got from the `get_article_text` in a folder *article_text* with file names as *URL_ID* given in `Output Data Structure.xlsx`.\n",
    "    * At last we traverse through all the URL's from the column URL of `Output Data Structure.xlsx` and get title and text of all the articles and stored them in *article_text* directory.\n",
    "    * We only get the text from 112 articles and 2 articles ( 11668.0.txt and 11668.0.txt ) both doesnt contain any text.\n",
    "\n",
    "    \n",
    "                        **----------------------- Data Extraction Ends Here --------------------------**\n",
    "                        \n",
    "2) **Data Analysis**\n",
    "\n",
    "> Data Analysis is divided into 2 parts in my program files....  \n",
    "        2.1) Removal of StopWords from Article Text   \n",
    "        2.2) Text Analysis\n",
    "        \n",
    "2.1. **Removal of StopWords from Article Text**\n",
    "  * Program for Removal of StopWords is in file `Removal_of_StopWords.py`\n",
    "  * start going through this program from `get_article_text`. This function returns the text of article from `article_text` directory.\n",
    "  * Then we tokenize this text we get from `get_article_text` using `tokenize_article_text` function.\n",
    "  * After this we need stopWords provided in the Assaignment in the `StopWords` directory.We get this stopwords using the function `get_stopwords`.\n",
    "  * Now , we have everything to remove stopwords from the article_text.Thus, this is performed by `clean_article_text` function.This function cleans all the 112 articles text from the stopwords provided to us.\n",
    "  * In the last lines of program, we called all the above functions to actually implement stopwords Removal from all the articles texts.\n",
    "  \n",
    "                    **----------------------- Removal of StopWords Ends Here --------------------------**\n",
    "                    \n",
    "2.2. **Text Analysis**\n",
    "    \n",
    "   * Program for Text Analysis is written in file `Text_Analysis`.\n",
    "   * Go to file `Text Analysis.docx`, this file contain all the steps to be followed to do text analysis on a text.\n",
    "   * created a pandas dataframe for file `data/Output Data Structure.xlsx - Sheet1.csv` named `df`.\n",
    "   * Now , we need to calculate variables provided in `Text Analysis.docx` and fill these values in columns of `df`.\n",
    "   * Thats why i created a function named `add_df_col_values` , this function takes the name of column to be filled and name of the function that is going to fill this column.\n",
    "   * Now, look every other function (other than `add_df_col_values`) is for calculating a certain variable, and the variable it is calculating is suggested in the name of the function itself.\n",
    "   * At last we called these functions on all the articles iteratively and after filling the `df` , I updated this `data/Output Data Structure.xlsx - Sheet1.csv`.\n",
    "   \n",
    "   \n",
    "   \n",
    "                   **----------------------- Text Analysis Ends Here --------------------------**\n",
    "\n",
    "**The End‚úåÔ∏èü•∑**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2dffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
