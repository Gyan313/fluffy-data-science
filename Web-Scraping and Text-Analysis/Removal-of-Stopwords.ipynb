{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709bdf9c",
   "metadata": {},
   "source": [
    "## Remove the stopwords from articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d07379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/Output Data Structure.xlsx - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3cb9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./articles_text/10282.6.txt\",\"r\",encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988e4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0202724c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Will',\n",
       " 'AI',\n",
       " 'Replace',\n",
       " 'Us',\n",
       " 'or',\n",
       " 'Work',\n",
       " 'With',\n",
       " 'Us',\n",
       " '?',\n",
       " '“',\n",
       " 'Machine',\n",
       " 'intelligence',\n",
       " 'is',\n",
       " 'the',\n",
       " 'last',\n",
       " 'invention',\n",
       " 'that',\n",
       " 'humanity',\n",
       " 'will',\n",
       " 'ever',\n",
       " 'need',\n",
       " 'to',\n",
       " 'make',\n",
       " '”',\n",
       " 'Nick',\n",
       " 'Bostrom',\n",
       " 'To',\n",
       " 'put',\n",
       " 'it',\n",
       " 'frankly',\n",
       " ',',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'will',\n",
       " 'eventually',\n",
       " 'replace',\n",
       " 'jobs',\n",
       " '.',\n",
       " 'Workers',\n",
       " 'in',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'industries',\n",
       " ',',\n",
       " 'from',\n",
       " 'healthcare',\n",
       " 'to',\n",
       " 'agriculture',\n",
       " 'and',\n",
       " 'manufacturing',\n",
       " ',',\n",
       " 'should',\n",
       " 'expect',\n",
       " 'to',\n",
       " 'witness',\n",
       " 'hiring',\n",
       " 'disruptions',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " '.',\n",
       " 'If',\n",
       " 'history',\n",
       " 'has',\n",
       " 'taught',\n",
       " 'us',\n",
       " 'anything',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'disruptive',\n",
       " 'paradigm-shifting',\n",
       " 'business',\n",
       " 'ideas',\n",
       " 'not',\n",
       " 'only',\n",
       " 'make',\n",
       " 'a',\n",
       " 'fortune',\n",
       " 'for',\n",
       " 'the',\n",
       " 'innovators',\n",
       " ',',\n",
       " 'but',\n",
       " 'they',\n",
       " 'also',\n",
       " 'build',\n",
       " 'the',\n",
       " 'groundwork',\n",
       " 'for',\n",
       " 'new',\n",
       " 'business',\n",
       " 'models',\n",
       " ',',\n",
       " 'market',\n",
       " 'entrants',\n",
       " ',',\n",
       " 'and',\n",
       " 'job',\n",
       " 'opportunities',\n",
       " 'which',\n",
       " 'will',\n",
       " 'inevitably',\n",
       " 'follow',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'true',\n",
       " 'that',\n",
       " 'robots',\n",
       " 'today',\n",
       " 'or',\n",
       " 'in',\n",
       " 'future',\n",
       " 'will',\n",
       " 'eventually',\n",
       " 'replace',\n",
       " 'humans',\n",
       " 'for',\n",
       " 'many',\n",
       " 'jobs',\n",
       " ',',\n",
       " 'but',\n",
       " 'so',\n",
       " 'did',\n",
       " 'innovative',\n",
       " 'farming',\n",
       " 'equipment',\n",
       " 'for',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'horses',\n",
       " 'during',\n",
       " 'the',\n",
       " 'industrial',\n",
       " 'revolution',\n",
       " '.',\n",
       " 'But',\n",
       " 'that',\n",
       " 'does',\n",
       " 'not',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'our',\n",
       " 'jobs',\n",
       " 'as',\n",
       " 'humans',\n",
       " 'will',\n",
       " 'end',\n",
       " 'here',\n",
       " '.',\n",
       " 'We',\n",
       " ',',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " ',',\n",
       " 'will',\n",
       " 'be',\n",
       " 'required',\n",
       " 'to',\n",
       " 'generate',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'value',\n",
       " 'in',\n",
       " 'whole',\n",
       " 'new',\n",
       " 'ways',\n",
       " 'for',\n",
       " 'entirely',\n",
       " 'new',\n",
       " 'business',\n",
       " 'models',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'these',\n",
       " 'changes',\n",
       " '.',\n",
       " 'According',\n",
       " 'to',\n",
       " '71',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'businesses',\n",
       " 'worldwide',\n",
       " ',',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'can',\n",
       " 'help',\n",
       " 'people',\n",
       " 'overcome',\n",
       " 'critical',\n",
       " 'and',\n",
       " 'challenging',\n",
       " 'problems',\n",
       " 'and',\n",
       " 'live',\n",
       " 'better',\n",
       " 'lives',\n",
       " '.',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'consultants',\n",
       " 'at',\n",
       " 'work',\n",
       " 'will',\n",
       " 'be',\n",
       " 'more',\n",
       " 'or',\n",
       " 'equally',\n",
       " 'fair',\n",
       " ',',\n",
       " 'according',\n",
       " 'to',\n",
       " 'a',\n",
       " 'whopping',\n",
       " '83',\n",
       " '%',\n",
       " 'of',\n",
       " 'corporate',\n",
       " 'leaders',\n",
       " '.',\n",
       " 'These',\n",
       " 'results',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'steadily',\n",
       " 'extending',\n",
       " 'its',\n",
       " 'measures',\n",
       " ',',\n",
       " 'yielding',\n",
       " 'societal',\n",
       " 'benefits',\n",
       " 'and',\n",
       " 'allowing',\n",
       " 'citizens',\n",
       " 'to',\n",
       " 'live',\n",
       " 'more',\n",
       " 'fulfilling',\n",
       " 'lives',\n",
       " '.',\n",
       " 'Increase',\n",
       " 'in',\n",
       " 'Automation',\n",
       " 'and',\n",
       " 'Jobs',\n",
       " 'where',\n",
       " 'humans',\n",
       " 'can',\n",
       " '’',\n",
       " 't',\n",
       " 'compete',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'Industry',\n",
       " '4.0',\n",
       " ',',\n",
       " 'businesses',\n",
       " 'are',\n",
       " 'moving',\n",
       " 'at',\n",
       " 'a',\n",
       " 'fast',\n",
       " 'pace',\n",
       " 'towards',\n",
       " 'automation',\n",
       " ',',\n",
       " 'be',\n",
       " 'it',\n",
       " 'any',\n",
       " 'type',\n",
       " 'of',\n",
       " 'industry',\n",
       " '.',\n",
       " 'In',\n",
       " '2013',\n",
       " ',',\n",
       " 'researchers',\n",
       " 'at',\n",
       " 'oxford',\n",
       " 'university',\n",
       " 'did',\n",
       " 'a',\n",
       " 'study',\n",
       " 'on',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'work',\n",
       " '.',\n",
       " 'They',\n",
       " 'concluded',\n",
       " 'that',\n",
       " 'almost',\n",
       " 'one',\n",
       " 'in',\n",
       " 'every',\n",
       " 'two',\n",
       " 'jobs',\n",
       " 'have',\n",
       " 'a',\n",
       " 'high',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'being',\n",
       " 'automated',\n",
       " 'by',\n",
       " 'machines',\n",
       " '.',\n",
       " 'Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'this',\n",
       " 'disruption',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'the',\n",
       " 'most',\n",
       " 'powerful',\n",
       " 'branch',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'It',\n",
       " 'allows',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data',\n",
       " 'and',\n",
       " 'mimic',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'things',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'can',\n",
       " 'do',\n",
       " '.',\n",
       " 'A',\n",
       " 'research',\n",
       " 'was',\n",
       " 'conducted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'employees',\n",
       " 'of',\n",
       " 'Kaggle',\n",
       " 'wherein',\n",
       " 'an',\n",
       " 'algorithm',\n",
       " 'was',\n",
       " 'to',\n",
       " 'be',\n",
       " 'created',\n",
       " 'to',\n",
       " 'take',\n",
       " 'images',\n",
       " 'of',\n",
       " 'a',\n",
       " 'human',\n",
       " 'eye',\n",
       " 'and',\n",
       " 'diagnose',\n",
       " 'an',\n",
       " 'eye',\n",
       " 'disease',\n",
       " 'known',\n",
       " 'as',\n",
       " 'diabetic',\n",
       " 'retinopathy',\n",
       " '.',\n",
       " 'Here',\n",
       " ',',\n",
       " 'the',\n",
       " 'winning',\n",
       " 'algorithm',\n",
       " 'could',\n",
       " 'match',\n",
       " 'the',\n",
       " 'diagnosis',\n",
       " 'given',\n",
       " 'by',\n",
       " 'human',\n",
       " 'ophthalmologists',\n",
       " '.',\n",
       " 'Another',\n",
       " 'study',\n",
       " 'was',\n",
       " 'conducted',\n",
       " 'wherein',\n",
       " 'an',\n",
       " 'algorithm',\n",
       " 'should',\n",
       " 'be',\n",
       " 'created',\n",
       " 'to',\n",
       " 'grade',\n",
       " 'high',\n",
       " 'school',\n",
       " 'essays',\n",
       " '.',\n",
       " 'Here',\n",
       " 'too',\n",
       " ',',\n",
       " 'the',\n",
       " 'winning',\n",
       " 'algorithm',\n",
       " 'could',\n",
       " 'match',\n",
       " 'the',\n",
       " 'grade',\n",
       " 'given',\n",
       " 'by',\n",
       " 'human',\n",
       " 'teachers',\n",
       " '.',\n",
       " 'Thus',\n",
       " ',',\n",
       " 'we',\n",
       " 'can',\n",
       " 'safely',\n",
       " 'conclude',\n",
       " 'that',\n",
       " 'given',\n",
       " 'the',\n",
       " 'right',\n",
       " 'data',\n",
       " ',',\n",
       " 'machines',\n",
       " 'can',\n",
       " 'easily',\n",
       " 'outperform',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'in',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'these',\n",
       " '.',\n",
       " 'A',\n",
       " 'teacher',\n",
       " 'might',\n",
       " 'read',\n",
       " '10,000',\n",
       " 'essays',\n",
       " 'over',\n",
       " 'a',\n",
       " '40-year',\n",
       " 'career',\n",
       " ';',\n",
       " 'an',\n",
       " 'ophthalmologist',\n",
       " 'might',\n",
       " 'see',\n",
       " '50,000',\n",
       " 'eyes',\n",
       " 'but',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'can',\n",
       " 'read',\n",
       " 'a',\n",
       " 'million',\n",
       " 'essays',\n",
       " 'and',\n",
       " 'see',\n",
       " 'a',\n",
       " 'million',\n",
       " 'eyes',\n",
       " 'within',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'Thus',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'convenient',\n",
       " 'to',\n",
       " 'conclude',\n",
       " 'that',\n",
       " 'we',\n",
       " 'have',\n",
       " 'no',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'competing',\n",
       " 'with',\n",
       " 'machines',\n",
       " 'on',\n",
       " 'frequent',\n",
       " ',',\n",
       " 'high',\n",
       " 'volume',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'Tasks',\n",
       " 'where',\n",
       " 'machines',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " 'work',\n",
       " 'But',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tasks',\n",
       " 'where',\n",
       " 'human',\n",
       " 'beings',\n",
       " 'have',\n",
       " 'an',\n",
       " 'upper',\n",
       " 'hand',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " 'is',\n",
       " ',',\n",
       " 'in',\n",
       " 'novel',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'Machines',\n",
       " 'can',\n",
       " '’',\n",
       " 't',\n",
       " 'handle',\n",
       " 'things',\n",
       " 'they',\n",
       " 'haven',\n",
       " '’',\n",
       " 't',\n",
       " 'seen',\n",
       " 'many',\n",
       " 'times',\n",
       " 'before',\n",
       " '.',\n",
       " 'The',\n",
       " 'fundamental',\n",
       " 'rule',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'learns',\n",
       " 'from',\n",
       " 'large',\n",
       " 'volumes',\n",
       " 'of',\n",
       " 'past',\n",
       " 'data',\n",
       " '.',\n",
       " 'But',\n",
       " 'humans',\n",
       " 'don',\n",
       " '’',\n",
       " 't',\n",
       " ';',\n",
       " 'we',\n",
       " 'have',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'seemingly',\n",
       " 'connecting',\n",
       " 'disparate',\n",
       " 'threads',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'problems',\n",
       " 'we',\n",
       " 'haven',\n",
       " '’',\n",
       " 't',\n",
       " 'seen',\n",
       " 'before',\n",
       " '.',\n",
       " 'Percy',\n",
       " 'Spencer',\n",
       " 'was',\n",
       " 'a',\n",
       " 'physicist',\n",
       " 'working',\n",
       " 'on',\n",
       " 'radar',\n",
       " 'during',\n",
       " 'world',\n",
       " 'war',\n",
       " '2',\n",
       " 'where',\n",
       " 'he',\n",
       " 'noticed',\n",
       " 'that',\n",
       " 'the',\n",
       " 'magnetron',\n",
       " 'was',\n",
       " 'melting',\n",
       " 'his',\n",
       " 'chocolate',\n",
       " 'bar',\n",
       " '.',\n",
       " 'Here',\n",
       " ',',\n",
       " 'he',\n",
       " 'was',\n",
       " 'able',\n",
       " 'to',\n",
       " 'connect',\n",
       " 'his',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'electromagnetic',\n",
       " 'radiation',\n",
       " 'with',\n",
       " 'his',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'cooking',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'invent',\n",
       " 'the',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " '.',\n",
       " 'Now',\n",
       " 'this',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'cross',\n",
       " 'pollination',\n",
       " 'happens',\n",
       " 'to',\n",
       " 'each',\n",
       " 'one',\n",
       " 'of',\n",
       " 'us',\n",
       " 'several',\n",
       " 'times',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " '.',\n",
       " 'Thus',\n",
       " ',',\n",
       " 'machines',\n",
       " 'can',\n",
       " 'not',\n",
       " 'compete',\n",
       " 'with',\n",
       " 'us',\n",
       " 'when',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'tackling',\n",
       " 'novel',\n",
       " 'situations',\n",
       " '.',\n",
       " 'Now',\n",
       " 'as',\n",
       " 'we',\n",
       " 'all',\n",
       " 'know',\n",
       " 'that',\n",
       " 'around',\n",
       " '92',\n",
       " '%',\n",
       " 'of',\n",
       " 'talented',\n",
       " 'professionals',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'soft',\n",
       " 'skills',\n",
       " 'such',\n",
       " 'as',\n",
       " 'human',\n",
       " 'interactions',\n",
       " 'and',\n",
       " 'fostering',\n",
       " 'relationships',\n",
       " 'matter',\n",
       " 'much',\n",
       " 'more',\n",
       " 'than',\n",
       " 'hard',\n",
       " 'skills',\n",
       " 'in',\n",
       " 'being',\n",
       " 'successful',\n",
       " 'in',\n",
       " 'managing',\n",
       " 'a',\n",
       " 'workplace',\n",
       " '.',\n",
       " 'Perhaps',\n",
       " ',',\n",
       " 'these',\n",
       " 'are',\n",
       " 'the',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'tasks',\n",
       " 'that',\n",
       " 'machines',\n",
       " 'can',\n",
       " 'never',\n",
       " 'compete',\n",
       " 'with',\n",
       " 'humans',\n",
       " 'at',\n",
       " '.',\n",
       " 'Also',\n",
       " ',',\n",
       " 'creative',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'the',\n",
       " 'copy',\n",
       " 'behind',\n",
       " 'a',\n",
       " 'marketing',\n",
       " 'campaign',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'grab',\n",
       " 'customers',\n",
       " '’',\n",
       " 'attention',\n",
       " 'and',\n",
       " 'will',\n",
       " 'have',\n",
       " 'to',\n",
       " 'stand',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'crowd',\n",
       " '.',\n",
       " 'Business',\n",
       " 'strategy',\n",
       " 'means',\n",
       " 'finding',\n",
       " 'gaps',\n",
       " 'in',\n",
       " 'the',\n",
       " 'market',\n",
       " 'and',\n",
       " 'accordingly',\n",
       " 'working',\n",
       " 'on',\n",
       " 'them',\n",
       " '.',\n",
       " 'Since',\n",
       " 'machines',\n",
       " 'can',\n",
       " 'not',\n",
       " 'outperform',\n",
       " 'humans',\n",
       " 'in',\n",
       " 'novel',\n",
       " 'tasks',\n",
       " ',',\n",
       " 'it',\n",
       " 'will',\n",
       " 'be',\n",
       " 'humans',\n",
       " 'who',\n",
       " 'would',\n",
       " 'be',\n",
       " 'creating',\n",
       " 'these',\n",
       " 'campaigns',\n",
       " 'and',\n",
       " 'strategies',\n",
       " '.',\n",
       " 'Human',\n",
       " 'contact',\n",
       " 'would',\n",
       " 'be',\n",
       " 'essential',\n",
       " 'in',\n",
       " 'care-giving',\n",
       " 'and',\n",
       " 'educational-related',\n",
       " 'work',\n",
       " 'responsibilities',\n",
       " ',',\n",
       " 'and',\n",
       " 'technology',\n",
       " 'would',\n",
       " 'take',\n",
       " 'a',\n",
       " 'backseat',\n",
       " '.',\n",
       " 'Health',\n",
       " 'screenings',\n",
       " 'and',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'face-to-face',\n",
       " 'communication',\n",
       " 'would',\n",
       " 'advocate',\n",
       " 'for',\n",
       " 'human',\n",
       " 'contact',\n",
       " ',',\n",
       " 'with',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'playing',\n",
       " 'a',\n",
       " 'supporting',\n",
       " 'role',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'what',\n",
       " 'does',\n",
       " 'this',\n",
       " 'mean',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'work',\n",
       " '?',\n",
       " 'The',\n",
       " 'future',\n",
       " 'state',\n",
       " 'of',\n",
       " 'any',\n",
       " 'single',\n",
       " 'job',\n",
       " 'lies',\n",
       " 'in',\n",
       " 'the',\n",
       " 'answer',\n",
       " 'to',\n",
       " 'one',\n",
       " 'single',\n",
       " 'question',\n",
       " ':',\n",
       " 'to',\n",
       " 'what',\n",
       " 'extent',\n",
       " 'is',\n",
       " 'the',\n",
       " 'job',\n",
       " 'reducible',\n",
       " 'to',\n",
       " 'tackling',\n",
       " 'frequent',\n",
       " 'high-volume',\n",
       " 'tasks',\n",
       " 'and',\n",
       " 'to',\n",
       " 'what',\n",
       " 'extent',\n",
       " 'does',\n",
       " 'it',\n",
       " 'involve',\n",
       " 'tackling',\n",
       " 'novel',\n",
       " 'situations',\n",
       " '?',\n",
       " 'Today',\n",
       " 'machines',\n",
       " 'diagnose',\n",
       " 'diseases',\n",
       " 'and',\n",
       " 'grade',\n",
       " 'exam',\n",
       " 'papers',\n",
       " ',',\n",
       " 'over',\n",
       " 'the',\n",
       " 'coming',\n",
       " 'years',\n",
       " 'they',\n",
       " '’',\n",
       " 're',\n",
       " 'going',\n",
       " 'to',\n",
       " 'conduct',\n",
       " 'audits',\n",
       " ',',\n",
       " 'they',\n",
       " '’',\n",
       " 're',\n",
       " 'going',\n",
       " 'to',\n",
       " 'read',\n",
       " 'boilerplate',\n",
       " 'from',\n",
       " 'legal',\n",
       " 'contracts',\n",
       " '.',\n",
       " 'But',\n",
       " 'does',\n",
       " 'that',\n",
       " 'mean',\n",
       " 'we',\n",
       " '’',\n",
       " 're',\n",
       " 'not',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'needing',\n",
       " 'accountants',\n",
       " 'and',\n",
       " 'lawyers',\n",
       " '?',\n",
       " 'Wrong',\n",
       " '.',\n",
       " 'We',\n",
       " '’',\n",
       " 're',\n",
       " 'still',\n",
       " 'going',\n",
       " 'to',\n",
       " 'need',\n",
       " 'them',\n",
       " 'for',\n",
       " 'complex',\n",
       " 'tax',\n",
       " 'structuring',\n",
       " ',',\n",
       " 'for',\n",
       " 'path',\n",
       " 'breaking',\n",
       " 'litigation',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'only',\n",
       " 'get',\n",
       " 'tougher',\n",
       " 'to',\n",
       " 'get',\n",
       " 'these',\n",
       " 'jobs',\n",
       " 'as',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'will',\n",
       " 'shrink',\n",
       " 'their',\n",
       " 'ranks',\n",
       " '.',\n",
       " 'Amazon',\n",
       " 'has',\n",
       " 'recruited',\n",
       " 'more',\n",
       " 'than',\n",
       " '100,000',\n",
       " 'robots',\n",
       " 'in',\n",
       " 'its',\n",
       " 'warehouses',\n",
       " 'to',\n",
       " 'help',\n",
       " 'move',\n",
       " 'goods',\n",
       " 'and',\n",
       " 'products',\n",
       " 'around',\n",
       " 'more',\n",
       " 'effectively',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea86eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text_without_sw = [words for words in tokenized_words if words not in stopwords.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6df14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all the given stop_words from the articles text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14939bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) removing all the auditor stopwords\n",
    "with open(\"./StopWords-20231016T011953Z-001/StopWords/StopWords_Names.txt\",\"r\",encoding='utf-8') as file:\n",
    "    file = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file)):\n",
    "    file[i] = file[i].rstrip()\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef916748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a60690",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df661438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get stopwords....\n",
    "def get_stopwords(path:str)->list:\n",
    "    with open(path,\"rb\") as file:\n",
    "        file = file.readlines()\n",
    "        \n",
    "        # removing the newline char and other char non relevant char.\n",
    "        for i in range(len(file)):\n",
    "            file[i] = file[i].rstrip()\n",
    "            \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba72584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to change tokenized text...\n",
    "def clean_tokens(stopwords:list)->list:\n",
    "    cleaned_tokens = [word for word in tokenized_text if word.lower() not in stopwords]\n",
    "    print(len(cleaned_tokens),\"---> this is the length of tokens after cleaning\")\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st StopWords_Auditor\n",
    "StopWords_Auditor = clean_tokens(get_stopwords('./StopWords-20231016T011953Z-001/StopWords/StopWords_Auditor.txt'))\n",
    "tokenized_text = StopWords_Auditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530146cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd StopWords_Currencies.txt\n",
    "StopWords_Currencies = get_stopwords('./StopWords-20231016T011953Z-001/StopWords/StopWords_Currencies.txt')\n",
    "\n",
    "# clean_words is a function to clean \n",
    "def clean_words(StopWords):\n",
    "    # removing the byte 'b' from the words\n",
    "    n = 0\n",
    "    for word in StopWords:\n",
    "        StopWords[n] = word.decode('iso-8859-1')\n",
    "        n+=1\n",
    "\n",
    "    # removing the | and '\\n' char from the word.\n",
    "    n=0\n",
    "    for word in StopWords:\n",
    "        StopWords[n] = word.split(\"|\")[0].rstrip().lower()\n",
    "        n+=1\n",
    "    return StopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805af00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_text = clean_tokens(StopWords_Currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c94944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd StopWords_DatesandNumbers\n",
    "StopWords_DatesandNumbers = get_stopwords(\"./StopWords-20231016T011953Z-001/StopWords/StopWords_DatesandNumbers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80487751",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_DatesandNumbers = clean_words(StopWords_DatesandNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = clean_tokens(StopWords_DatesandNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d84c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th StopWords_Generic\n",
    "StopWords_Generic = get_stopwords(\"./StopWords-20231016T011953Z-001/StopWords/StopWords_Generic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Generic = clean_words(StopWords_Generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc351d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = clean_tokens(StopWords_Generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th StopWords_GenericLong\n",
    "StopWords_GenericLong = get_stopwords(\"./StopWords-20231016T011953Z-001/StopWords/StopWords_GenericLong.txt\")\n",
    "StopWords_GenericLong = clean_words(StopWords_GenericLong)\n",
    "tokenized_text = clean_tokens(StopWords_GenericLong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b62af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th StopWords_Geographic\n",
    "StopWords_Geographic = get_stopwords(\"./StopWords-20231016T011953Z-001/StopWords/StopWords_Geographic.txt\")\n",
    "StopWords_Geographic = clean_words(StopWords_Geographic)\n",
    "tokenized_text = clean_tokens(StopWords_Geographic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th StopWords_Names\n",
    "StopWords_Names = get_stopwords(\"./StopWords-20231016T011953Z-001/StopWords/StopWords_Names.txt\")\n",
    "StopWords_Names = clean_words(StopWords_Names)\n",
    "tokenized_text = clean_tokens(StopWords_Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d3548d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_text = [word for word in tokenized_text if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "n = 0\n",
    "for word in tokenized_text:\n",
    "    tokenized_text[n] = re.sub(r\"[^A-Za-z]\",\"\",word)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [word for word in tokenized_text if word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fd4ec",
   "metadata": {},
   "source": [
    "## Real code for Removal of Stopwords from all the articles is below......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfa2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./articles_text/10282.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1971 ---> this is no. of tokens we have from the article.....\n",
      "811 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/10282.6.txt✌️❤️\n",
      "./articles_text/10744.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1480 ---> this is no. of tokens we have from the article.....\n",
      "577 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/10744.4.txt✌️❤️\n",
      "./articles_text/11206.2.txt ---> this is the article we are cleaning right now...😎\n",
      "840 ---> this is no. of tokens we have from the article.....\n",
      "338 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/11206.2.txt✌️❤️\n",
      "./articles_text/12129.8.txt ---> this is the article we are cleaning right now...😎\n",
      "818 ---> this is no. of tokens we have from the article.....\n",
      "330 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/12129.8.txt✌️❤️\n",
      "./articles_text/123.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1847 ---> this is no. of tokens we have from the article.....\n",
      "833 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/123.0.txt✌️❤️\n",
      "./articles_text/12591.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1787 ---> this is no. of tokens we have from the article.....\n",
      "734 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/12591.6.txt✌️❤️\n",
      "./articles_text/13053.4.txt ---> this is the article we are cleaning right now...😎\n",
      "2367 ---> this is no. of tokens we have from the article.....\n",
      "1025 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/13053.4.txt✌️❤️\n",
      "./articles_text/13515.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1314 ---> this is no. of tokens we have from the article.....\n",
      "590 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/13515.2.txt✌️❤️\n",
      "./articles_text/13977.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1426 ---> this is no. of tokens we have from the article.....\n",
      "619 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/13977.0.txt✌️❤️\n",
      "./articles_text/14438.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1783 ---> this is no. of tokens we have from the article.....\n",
      "659 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/14438.8.txt✌️❤️\n",
      "./articles_text/14900.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1866 ---> this is no. of tokens we have from the article.....\n",
      "751 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/14900.6.txt✌️❤️\n",
      "./articles_text/15362.4.txt ---> this is the article we are cleaning right now...😎\n",
      "295 ---> this is no. of tokens we have from the article.....\n",
      "130 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/15362.4.txt✌️❤️\n",
      "./articles_text/15824.2.txt ---> this is the article we are cleaning right now...😎\n",
      "590 ---> this is no. of tokens we have from the article.....\n",
      "204 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/15824.2.txt✌️❤️\n",
      "./articles_text/16286.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1088 ---> this is no. of tokens we have from the article.....\n",
      "461 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/16286.0.txt✌️❤️\n",
      "./articles_text/16747.8.txt ---> this is the article we are cleaning right now...😎\n",
      "811 ---> this is no. of tokens we have from the article.....\n",
      "357 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/16747.8.txt✌️❤️\n",
      "./articles_text/17209.6.txt ---> this is the article we are cleaning right now...😎\n",
      "335 ---> this is no. of tokens we have from the article.....\n",
      "131 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/17209.6.txt✌️❤️\n",
      "./articles_text/18133.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1337 ---> this is no. of tokens we have from the article.....\n",
      "567 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/18133.2.txt✌️❤️\n",
      "./articles_text/18595.0.txt ---> this is the article we are cleaning right now...😎\n",
      "806 ---> this is no. of tokens we have from the article.....\n",
      "311 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/18595.0.txt✌️❤️\n",
      "./articles_text/19056.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1800 ---> this is no. of tokens we have from the article.....\n",
      "676 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/19056.8.txt✌️❤️\n",
      "./articles_text/19518.6.txt ---> this is the article we are cleaning right now...😎\n",
      "721 ---> this is no. of tokens we have from the article.....\n",
      "310 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/19518.6.txt✌️❤️\n",
      "./articles_text/19980.4.txt ---> this is the article we are cleaning right now...😎\n",
      "416 ---> this is no. of tokens we have from the article.....\n",
      "188 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/19980.4.txt✌️❤️\n",
      "./articles_text/20442.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1095 ---> this is no. of tokens we have from the article.....\n",
      "464 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/20442.2.txt✌️❤️\n",
      "./articles_text/20904.0.txt ---> this is the article we are cleaning right now...😎\n",
      "537 ---> this is no. of tokens we have from the article.....\n",
      "260 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/20904.0.txt✌️❤️\n",
      "./articles_text/21365.8.txt ---> this is the article we are cleaning right now...😎\n",
      "438 ---> this is no. of tokens we have from the article.....\n",
      "146 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/21365.8.txt✌️❤️\n",
      "./articles_text/21827.6.txt ---> this is the article we are cleaning right now...😎\n",
      "177 ---> this is no. of tokens we have from the article.....\n",
      "82 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/21827.6.txt✌️❤️\n",
      "./articles_text/22289.4.txt ---> this is the article we are cleaning right now...😎\n",
      "947 ---> this is no. of tokens we have from the article.....\n",
      "345 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/22289.4.txt✌️❤️\n",
      "./articles_text/22751.2.txt ---> this is the article we are cleaning right now...😎\n",
      "193 ---> this is no. of tokens we have from the article.....\n",
      "70 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/22751.2.txt✌️❤️\n",
      "./articles_text/23213.0.txt ---> this is the article we are cleaning right now...😎\n",
      "968 ---> this is no. of tokens we have from the article.....\n",
      "461 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/23213.0.txt✌️❤️\n",
      "./articles_text/2345.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1211 ---> this is no. of tokens we have from the article.....\n",
      "521 ---> this is the no. of tokens after cleaning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/2345.0.txt✌️❤️\n",
      "./articles_text/23674.8.txt ---> this is the article we are cleaning right now...😎\n",
      "593 ---> this is no. of tokens we have from the article.....\n",
      "235 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/23674.8.txt✌️❤️\n",
      "./articles_text/24136.6.txt ---> this is the article we are cleaning right now...😎\n",
      "575 ---> this is no. of tokens we have from the article.....\n",
      "210 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/24136.6.txt✌️❤️\n",
      "./articles_text/24598.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1972 ---> this is no. of tokens we have from the article.....\n",
      "816 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/24598.4.txt✌️❤️\n",
      "./articles_text/25060.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1707 ---> this is no. of tokens we have from the article.....\n",
      "623 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/25060.2.txt✌️❤️\n",
      "./articles_text/25522.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1675 ---> this is no. of tokens we have from the article.....\n",
      "647 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/25522.0.txt✌️❤️\n",
      "./articles_text/25983.8.txt ---> this is the article we are cleaning right now...😎\n",
      "882 ---> this is no. of tokens we have from the article.....\n",
      "293 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/25983.8.txt✌️❤️\n",
      "./articles_text/26445.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1484 ---> this is no. of tokens we have from the article.....\n",
      "476 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/26445.6.txt✌️❤️\n",
      "./articles_text/26907.4.txt ---> this is the article we are cleaning right now...😎\n",
      "527 ---> this is no. of tokens we have from the article.....\n",
      "167 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/26907.4.txt✌️❤️\n",
      "./articles_text/27369.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1737 ---> this is no. of tokens we have from the article.....\n",
      "709 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/27369.2.txt✌️❤️\n",
      "./articles_text/27831.0.txt ---> this is the article we are cleaning right now...😎\n",
      "873 ---> this is no. of tokens we have from the article.....\n",
      "308 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/27831.0.txt✌️❤️\n",
      "./articles_text/28292.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1280 ---> this is no. of tokens we have from the article.....\n",
      "458 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/28292.8.txt✌️❤️\n",
      "./articles_text/28754.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1256 ---> this is no. of tokens we have from the article.....\n",
      "559 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/28754.6.txt✌️❤️\n",
      "./articles_text/2893.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1304 ---> this is no. of tokens we have from the article.....\n",
      "580 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/2893.8.txt✌️❤️\n",
      "./articles_text/29216.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1598 ---> this is no. of tokens we have from the article.....\n",
      "699 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/29216.4.txt✌️❤️\n",
      "./articles_text/29678.2.txt ---> this is the article we are cleaning right now...😎\n",
      "934 ---> this is no. of tokens we have from the article.....\n",
      "338 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/29678.2.txt✌️❤️\n",
      "./articles_text/30140.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1255 ---> this is no. of tokens we have from the article.....\n",
      "454 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/30140.0.txt✌️❤️\n",
      "./articles_text/30601.8.txt ---> this is the article we are cleaning right now...😎\n",
      "563 ---> this is no. of tokens we have from the article.....\n",
      "248 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/30601.8.txt✌️❤️\n",
      "./articles_text/31063.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1324 ---> this is no. of tokens we have from the article.....\n",
      "578 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/31063.6.txt✌️❤️\n",
      "./articles_text/31525.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1864 ---> this is no. of tokens we have from the article.....\n",
      "762 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/31525.4.txt✌️❤️\n",
      "./articles_text/31987.2.txt ---> this is the article we are cleaning right now...😎\n",
      "3897 ---> this is no. of tokens we have from the article.....\n",
      "1823 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/31987.2.txt✌️❤️\n",
      "./articles_text/321.0.txt ---> this is the article we are cleaning right now...😎\n",
      "660 ---> this is no. of tokens we have from the article.....\n",
      "274 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/321.0.txt✌️❤️\n",
      "./articles_text/32449.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1941 ---> this is no. of tokens we have from the article.....\n",
      "775 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/32449.0.txt✌️❤️\n",
      "./articles_text/32910.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1688 ---> this is no. of tokens we have from the article.....\n",
      "805 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/32910.8.txt✌️❤️\n",
      "./articles_text/33372.6.txt ---> this is the article we are cleaning right now...😎\n",
      "141 ---> this is no. of tokens we have from the article.....\n",
      "50 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/33372.6.txt✌️❤️\n",
      "./articles_text/3355.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1210 ---> this is no. of tokens we have from the article.....\n",
      "561 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/3355.6.txt✌️❤️\n",
      "./articles_text/33834.4.txt ---> this is the article we are cleaning right now...😎\n",
      "614 ---> this is no. of tokens we have from the article.....\n",
      "271 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/33834.4.txt✌️❤️\n",
      "./articles_text/34296.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1895 ---> this is no. of tokens we have from the article.....\n",
      "644 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/34296.2.txt✌️❤️\n",
      "./articles_text/34758.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1871 ---> this is no. of tokens we have from the article.....\n",
      "757 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/34758.0.txt✌️❤️\n",
      "./articles_text/35219.8.txt ---> this is the article we are cleaning right now...😎\n",
      "878 ---> this is no. of tokens we have from the article.....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/35219.8.txt✌️❤️\n",
      "./articles_text/35681.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1365 ---> this is no. of tokens we have from the article.....\n",
      "606 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/35681.6.txt✌️❤️\n",
      "./articles_text/36143.4.txt ---> this is the article we are cleaning right now...😎\n",
      "988 ---> this is no. of tokens we have from the article.....\n",
      "417 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/36143.4.txt✌️❤️\n",
      "./articles_text/36605.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1333 ---> this is no. of tokens we have from the article.....\n",
      "497 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/36605.2.txt✌️❤️\n",
      "./articles_text/37067.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1172 ---> this is no. of tokens we have from the article.....\n",
      "460 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/37067.0.txt✌️❤️\n",
      "./articles_text/37528.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1466 ---> this is no. of tokens we have from the article.....\n",
      "687 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/37528.8.txt✌️❤️\n",
      "./articles_text/37990.6.txt ---> this is the article we are cleaning right now...😎\n",
      "164 ---> this is no. of tokens we have from the article.....\n",
      "57 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/37990.6.txt✌️❤️\n",
      "./articles_text/3817.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1945 ---> this is no. of tokens we have from the article.....\n",
      "844 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/3817.4.txt✌️❤️\n",
      "./articles_text/38452.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1230 ---> this is no. of tokens we have from the article.....\n",
      "498 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/38452.4.txt✌️❤️\n",
      "./articles_text/38914.2.txt ---> this is the article we are cleaning right now...😎\n",
      "727 ---> this is no. of tokens we have from the article.....\n",
      "326 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/38914.2.txt✌️❤️\n",
      "./articles_text/39376.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1039 ---> this is no. of tokens we have from the article.....\n",
      "459 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/39376.0.txt✌️❤️\n",
      "./articles_text/39837.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1099 ---> this is no. of tokens we have from the article.....\n",
      "371 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/39837.8.txt✌️❤️\n",
      "./articles_text/40299.6.txt ---> this is the article we are cleaning right now...😎\n",
      "435 ---> this is no. of tokens we have from the article.....\n",
      "210 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/40299.6.txt✌️❤️\n",
      "./articles_text/40761.4.txt ---> this is the article we are cleaning right now...😎\n",
      "680 ---> this is no. of tokens we have from the article.....\n",
      "277 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/40761.4.txt✌️❤️\n",
      "./articles_text/41223.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1134 ---> this is no. of tokens we have from the article.....\n",
      "512 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/41223.2.txt✌️❤️\n",
      "./articles_text/41685.0.txt ---> this is the article we are cleaning right now...😎\n",
      "801 ---> this is no. of tokens we have from the article.....\n",
      "235 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/41685.0.txt✌️❤️\n",
      "./articles_text/42146.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1175 ---> this is no. of tokens we have from the article.....\n",
      "498 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/42146.8.txt✌️❤️\n",
      "./articles_text/42608.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1150 ---> this is no. of tokens we have from the article.....\n",
      "460 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/42608.6.txt✌️❤️\n",
      "./articles_text/4279.2.txt ---> this is the article we are cleaning right now...😎\n",
      "603 ---> this is no. of tokens we have from the article.....\n",
      "219 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/4279.2.txt✌️❤️\n",
      "./articles_text/43070.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1146 ---> this is no. of tokens we have from the article.....\n",
      "513 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/43070.4.txt✌️❤️\n",
      "./articles_text/432.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1364 ---> this is no. of tokens we have from the article.....\n",
      "641 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/432.0.txt✌️❤️\n",
      "./articles_text/4321.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1364 ---> this is no. of tokens we have from the article.....\n",
      "641 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/4321.0.txt✌️❤️\n",
      "./articles_text/43532.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1375 ---> this is no. of tokens we have from the article.....\n",
      "564 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/43532.2.txt✌️❤️\n",
      "./articles_text/43994.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1723 ---> this is no. of tokens we have from the article.....\n",
      "719 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/43994.0.txt✌️❤️\n",
      "./articles_text/44455.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1757 ---> this is no. of tokens we have from the article.....\n",
      "802 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/44455.8.txt✌️❤️\n",
      "./articles_text/44917.6.txt ---> this is the article we are cleaning right now...😎\n",
      "890 ---> this is no. of tokens we have from the article.....\n",
      "328 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/44917.6.txt✌️❤️\n",
      "./articles_text/45379.4.txt ---> this is the article we are cleaning right now...😎\n",
      "2150 ---> this is no. of tokens we have from the article.....\n",
      "942 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/45379.4.txt✌️❤️\n",
      "./articles_text/45841.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1396 ---> this is no. of tokens we have from the article.....\n",
      "492 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/45841.2.txt✌️❤️\n",
      "./articles_text/46303.0.txt ---> this is the article we are cleaning right now...😎\n",
      "378 ---> this is no. of tokens we have from the article.....\n",
      "184 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/46303.0.txt✌️❤️\n",
      "./articles_text/46764.8.txt ---> this is the article we are cleaning right now...😎\n",
      "2388 ---> this is no. of tokens we have from the article.....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/46764.8.txt✌️❤️\n",
      "./articles_text/47226.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1719 ---> this is no. of tokens we have from the article.....\n",
      "549 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/47226.6.txt✌️❤️\n",
      "./articles_text/4741.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1160 ---> this is no. of tokens we have from the article.....\n",
      "517 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/4741.0.txt✌️❤️\n",
      "./articles_text/47688.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1773 ---> this is no. of tokens we have from the article.....\n",
      "705 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/47688.4.txt✌️❤️\n",
      "./articles_text/48150.2.txt ---> this is the article we are cleaning right now...😎\n",
      "883 ---> this is no. of tokens we have from the article.....\n",
      "342 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/48150.2.txt✌️❤️\n",
      "./articles_text/48612.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1918 ---> this is no. of tokens we have from the article.....\n",
      "918 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/48612.0.txt✌️❤️\n",
      "./articles_text/49073.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1892 ---> this is no. of tokens we have from the article.....\n",
      "778 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/49073.8.txt✌️❤️\n",
      "./articles_text/49535.6.txt ---> this is the article we are cleaning right now...😎\n",
      "955 ---> this is no. of tokens we have from the article.....\n",
      "396 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/49535.6.txt✌️❤️\n",
      "./articles_text/49997.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1338 ---> this is no. of tokens we have from the article.....\n",
      "581 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/49997.4.txt✌️❤️\n",
      "./articles_text/50459.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1094 ---> this is no. of tokens we have from the article.....\n",
      "396 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/50459.2.txt✌️❤️\n",
      "./articles_text/50921.0.txt ---> this is the article we are cleaning right now...😎\n",
      "606 ---> this is no. of tokens we have from the article.....\n",
      "274 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/50921.0.txt✌️❤️\n",
      "./articles_text/51382.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1201 ---> this is no. of tokens we have from the article.....\n",
      "533 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/51382.8.txt✌️❤️\n",
      "./articles_text/51844.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1940 ---> this is no. of tokens we have from the article.....\n",
      "881 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/51844.6.txt✌️❤️\n",
      "./articles_text/5202.8.txt ---> this is the article we are cleaning right now...😎\n",
      "459 ---> this is no. of tokens we have from the article.....\n",
      "167 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/5202.8.txt✌️❤️\n",
      "./articles_text/52306.4.txt ---> this is the article we are cleaning right now...😎\n",
      "1582 ---> this is no. of tokens we have from the article.....\n",
      "642 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/52306.4.txt✌️❤️\n",
      "./articles_text/52768.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1121 ---> this is no. of tokens we have from the article.....\n",
      "522 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/52768.2.txt✌️❤️\n",
      "./articles_text/5664.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1530 ---> this is no. of tokens we have from the article.....\n",
      "584 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/5664.6.txt✌️❤️\n",
      "./articles_text/6126.4.txt ---> this is the article we are cleaning right now...😎\n",
      "633 ---> this is no. of tokens we have from the article.....\n",
      "271 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/6126.4.txt✌️❤️\n",
      "./articles_text/6588.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1161 ---> this is no. of tokens we have from the article.....\n",
      "452 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/6588.2.txt✌️❤️\n",
      "./articles_text/7050.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1013 ---> this is no. of tokens we have from the article.....\n",
      "460 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/7050.0.txt✌️❤️\n",
      "./articles_text/7511.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1477 ---> this is no. of tokens we have from the article.....\n",
      "562 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/7511.8.txt✌️❤️\n",
      "./articles_text/7973.6.txt ---> this is the article we are cleaning right now...😎\n",
      "1257 ---> this is no. of tokens we have from the article.....\n",
      "563 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/7973.6.txt✌️❤️\n",
      "./articles_text/8435.4.txt ---> this is the article we are cleaning right now...😎\n",
      "2005 ---> this is no. of tokens we have from the article.....\n",
      "973 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/8435.4.txt✌️❤️\n",
      "./articles_text/8897.2.txt ---> this is the article we are cleaning right now...😎\n",
      "1639 ---> this is no. of tokens we have from the article.....\n",
      "589 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/8897.2.txt✌️❤️\n",
      "./articles_text/9359.0.txt ---> this is the article we are cleaning right now...😎\n",
      "1914 ---> this is no. of tokens we have from the article.....\n",
      "829 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/9359.0.txt✌️❤️\n",
      "./articles_text/9820.8.txt ---> this is the article we are cleaning right now...😎\n",
      "1827 ---> this is no. of tokens we have from the article.....\n",
      "655 ---> this is the no. of tokens after cleaning\n",
      "The cleaned tokens of articles has been written in file----./Cleaned-Articles/9820.8.txt✌️❤️\n",
      "Happy!Happy!Happy, everything happened seemlessly.......🙃😊🤪\n"
     ]
    }
   ],
   "source": [
    "# creating fucntions to do stopwords cleaning....\n",
    "\n",
    "# import all the modules required here....\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# now we need to remove all the stop words from article_text\n",
    "def clean_article_text(article_file_path:str)->list:\n",
    "    \n",
    "    print(f\"{article_file_path} ---> this is the article we are cleaning right now...😎\")\n",
    "    article_text = get_article_text(article_file_path)\n",
    "\n",
    "    tokenized_text_words = tokenize_article_text(article_text)\n",
    "    print(f\"{len(tokenized_text_words)} ---> this is no. of tokens we have from the article.....\")\n",
    "    \n",
    "    # here removing stopwords from all the stopwords_files given.....\n",
    "    list_of_stopwords_file = os.listdir(\"./StopWords-20231016T011953Z-001/StopWords/\")\n",
    "    \n",
    "    for stopwords_file in list_of_stopwords_file:\n",
    "        stopwords = get_stopwords(f\"./StopWords-20231016T011953Z-001/StopWords/{stopwords_file}\")\n",
    "        tokenized_text_words = [word for word in tokenized_text_words if word.lower() not in stopwords]\n",
    "    \n",
    "    tokenized_text_words = [word for word in tokenized_text_words if word not in '\"#$&\\'(),-./:;[\\\\]^_`{|}~']\n",
    "    cleaned_tokens = tokenized_text_words\n",
    "    print(len(cleaned_tokens),\"---> this is the no. of tokens after cleaning\")\n",
    "    \n",
    "    # we cannot store the cleaned tokens in files here in this function because this will repeatedly overwrite these files\n",
    "    # everytime this function run...\n",
    "    return cleaned_tokens\n",
    "\n",
    "    \n",
    "# creating function to get text of articles for stopwords cleaning\n",
    "def get_article_text(path):\n",
    "    with open(path,\"r\",encoding='utf-8') as file:\n",
    "        article_text = file.read()\n",
    "    return article_text\n",
    "\n",
    "# tokenize the text we get from article.....\n",
    "def tokenize_article_text(article_text):\n",
    "    tokenized_words = word_tokenize(article_text)\n",
    "    return tokenized_words\n",
    "\n",
    "# removing all the stopwords(provided in Stopwords directory) from the tokenized_words from article....\n",
    "# getting stop words from files\n",
    "def get_stopwords(path:str)->list:\n",
    "    with open(path,\"rb\") as file:\n",
    "        file = file.readlines()\n",
    "\n",
    "    # removing the byte 'b' from the words\n",
    "    n = 0\n",
    "    for word in file:\n",
    "        file[n] = word.decode('iso-8859-1')\n",
    "        n+=1\n",
    "\n",
    "    # removing the | and '\\n' char from the word.\n",
    "    file = [word.split(\"|\")[0].rstrip().lower() for word in file]\n",
    "\n",
    "    return file\n",
    "\n",
    "# store these refined tokens of articles in files, name equal to those file\n",
    "def store_tokenized_clean_article(path,cleaned_article_tokens):\n",
    "    with open(path,\"w\",encoding='utf-8') as file:\n",
    "        file.write(\"\\n\".join(word for word in cleaned_article_tokens))\n",
    "    print(f\"The cleaned tokens of articles has been written in file----{path}✌️❤️\")\n",
    "\n",
    "    \n",
    "    \n",
    "# executing the above functions to clean the article text.....\n",
    "if __name__==\"__main__\":\n",
    "    list_of_articles = os.listdir(\"./articles_text/\")[1:]\n",
    "    list_of_stopwords_file = os.listdir(\"./StopWords-20231016T011953Z-001/StopWords/\")\n",
    "    \n",
    "    # implementing....\n",
    "    try:\n",
    "        for article_file in list_of_articles:\n",
    "            # creating file path to store cleaned tokens of article....\n",
    "            clean_token_file_path = f\"./Cleaned-Articles/{article_file}\"\n",
    "            \n",
    "            # calling function to clean the article text\n",
    "            cleaned_tokens = clean_article_text(article_file_path=f\"./articles_text/{article_file}\")\n",
    "            \n",
    "            # storing the cleaned tokens\n",
    "            store_tokenized_clean_article(clean_token_file_path,cleaned_tokens)\n",
    "        print(\"Happy!Happy!Happy, everything happened seemlessly.......🙃😊🤪\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d30b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd3002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60833a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a6854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08419fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b75d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
